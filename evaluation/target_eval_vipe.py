#!/usr/bin/env python3
"""
Evaluate World Models Performance using World Decoder (VIPE Version)

This script:
1. Processes videos generated by world models (veo3.1, veo3.1fast, sora2, etc.)
2. Extracts N evenly-spaced frames (default: 9) from each video
3. Creates a sampled video from the extracted frames
4. Runs VIPE SLAM on the sampled video to extract camera poses
5. Converts poses (tx, ty, tz, qx, qy, qz, qw format) to trajectory coordinates
6. Properly handles coordinate system conversion (OpenCV to top-down view)
7. Compares extracted trajectories with ground truth
8. Computes evaluation metrics (ADE, FDE, MR, SE, AC, Overall Score)
9. Generates comparison visualizations and reports

Key points:
- Extracts 9 evenly-spaced frames from each video for efficient SLAM processing
- VIPE outputs real metric values directly, so no scale factor computation is needed
- Uses extract_trajectory_from_poses() for proper pose-to-trajectory conversion
- Handles VIPE's internal pose representation: (N, 7) format [tx, ty, tz, qx, qy, qz, qw]
- Applies coordinate system transformations for top-down trajectory view

Directory structure expected:
- dataset/Target_samples/{segment_name}/ or dataset/Benchmark/{segment_name}/ - Ground truth data
  - {segment_name}_first_frame.png
  - {segment_name}_current_frame.png
  - {segment_name}_metadata.json
  - {segment_name}_trajectory.csv (ground truth trajectory)
  
- dataset/wm_videos/{model_name}/ - World model generated videos
  - {segment_name}.mp4
"""

import os
import sys
import glob
import json
import yaml
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm.auto import tqdm
import torch
import cv2
import argparse
import tempfile
import shutil

# Define project root and add paths
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# Add paths
sys.path.append(os.path.join(PROJECT_ROOT, 'models/vipe'))
sys.path.append(os.path.join(PROJECT_ROOT, 'pipelines/vipe'))
sys.path.append(os.path.join(PROJECT_ROOT, 'evaluation'))

from extrinsic_path.extract_extrinsic_path import (
    extract_trajectory_from_poses,
    load_ground_truth_trajectory
)

# Import trajectory metrics
from metrics import compute_all_metrics

# Import VIPE components
from vipe.streams.raw_mp4_stream import RawMp4Stream
from vipe.pipeline.default import DefaultAnnotationPipeline
from omegaconf import OmegaConf


def load_evaluation_config(config_path=None):
    """
    Load evaluation configuration from YAML file.
    
    Args:
        config_path: Path to config YAML file
        
    Returns:
        dict: Configuration dictionary
    """
    if config_path is None:
        config_path = os.path.join(PROJECT_ROOT, "evaluation/config/vggt_config.yaml")
        
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def find_world_model_videos(wm_videos_dir=None):
    """
    Find all world model videos organized by model type.
    
    Returns:
        dict: {model_name: {segment_name: video_path}}
    """
    if wm_videos_dir is None:
        wm_videos_dir = os.path.join(PROJECT_ROOT, "dataset/wm_videos")
        
    wm_videos = {}
    
    # Find all subdirectories in wm_videos (each is a model)
    if not os.path.exists(wm_videos_dir):
        print(f"Warning: World model videos directory not found: {wm_videos_dir}")
        return {}
        
    model_dirs = [d for d in glob.glob(os.path.join(wm_videos_dir, "*/")) if os.path.isdir(d)]
    
    for model_dir in model_dirs:
        model_name = os.path.basename(model_dir.rstrip('/'))
        wm_videos[model_name] = {}
        
        # Find all mp4 files in this model directory
        video_files = glob.glob(os.path.join(model_dir, "*.mp4"))
        
        for video_path in video_files:
            # Extract segment name from filename (remove .mp4 extension)
            segment_name = os.path.splitext(os.path.basename(video_path))[0]
            wm_videos[model_name][segment_name] = video_path
        
        print(f"Found {len(wm_videos[model_name])} videos for model: {model_name}")
    
    return wm_videos


def extract_n_frames_from_video(video_path, n_frames=9, output_dir=None):
    """
    Extract N evenly-spaced frames from a video.
    
    Args:
        video_path: Path to input video
        n_frames: Number of frames to extract (default: 9)
        output_dir: Directory to save frames (if None, uses temp directory)
        
    Returns:
        list: Paths to extracted frame images
    """
    # Open video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"Cannot open video: {video_path}")
    
    # Get video properties
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    if total_frames < n_frames:
        print(f"Warning: Video has {total_frames} frames, requesting {n_frames}")
        n_frames = total_frames
    
    # Calculate frame indices to extract (evenly spaced)
    frame_indices = np.linspace(0, total_frames - 1, n_frames, dtype=int)
    
    # Create output directory
    if output_dir is None:
        output_dir = tempfile.mkdtemp(prefix="vipe_frames_")
    os.makedirs(output_dir, exist_ok=True)
    
    frame_paths = []
    
    for i, frame_idx in enumerate(frame_indices):
        # Seek to frame
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            print(f"Warning: Could not read frame {frame_idx}")
            continue
        
        # Save frame
        frame_path = os.path.join(output_dir, f"frame_{i:04d}.png")
        cv2.imwrite(frame_path, frame)
        frame_paths.append(frame_path)
    
    cap.release()
    
    return frame_paths


def create_video_from_frames(frame_paths, output_video_path, fps=10):
    """
    Create a video from a list of frame images.
    
    Args:
        frame_paths: List of paths to frame images
        output_video_path: Path to output video file
        fps: Frames per second for output video
        
    Returns:
        str: Path to created video
    """
    if not frame_paths:
        raise ValueError("No frames provided")
    
    # Read first frame to get dimensions
    first_frame = cv2.imread(frame_paths[0])
    if first_frame is None:
        raise ValueError(f"Cannot read first frame: {frame_paths[0]}")
    
    height, width, _ = first_frame.shape
    
    # Create video writer
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
    
    # Write all frames
    for frame_path in frame_paths:
        frame = cv2.imread(frame_path)
        if frame is None:
            print(f"Warning: Could not read frame {frame_path}")
            continue
        out.write(frame)
    
    out.release()
    
    return output_video_path


def load_precomputed_vipe_poses(segment_name, vipe_results_dir=None):
    """
    Load pre-computed VIPE poses from results directory.
    
    Args:
        segment_name: Name of the segment
        vipe_results_dir: Directory containing VIPE results
        
    Returns:
        poses: numpy array of shape (n_frames, 7) containing camera poses
               Format: [tx, ty, tz, qx, qy, qz, qw] per frame
               None if not found
    """
    if vipe_results_dir is None:
        vipe_results_dir = os.path.join(PROJECT_ROOT, "models/vipe/vipe_results")
        
    # Try different possible naming patterns
    possible_names = [
        f"{segment_name}_video.npz",
        f"{segment_name}.npz",
        segment_name + ".npz"
    ]
    
    pose_dir = os.path.join(vipe_results_dir, "pose")
    
    for name in possible_names:
        pose_path = os.path.join(pose_dir, name)
        if os.path.exists(pose_path):
            try:
                data = np.load(pose_path)
                if 'data' in data:
                    poses = data['data']
                    # Ensure poses are in the correct format: (n_frames, 7)
                    if len(poses.shape) == 3 and poses.shape[0] == 1:
                        poses = poses[0]  # Remove batch dimension
                    print(f"  ✓ Loaded {len(poses)} pre-computed poses from VIPE results")
                    return poses
            except Exception as e:
                print(f"  Warning: Could not load {pose_path}: {e}")
                continue
    
    return None


def run_vipe_on_video_with_sampling(video_path, n_frames=9, output_dir=None, temp_dir=None):
    """
    Extract N frames from video, create sampled video, and run VIPE SLAM.
    
    This function:
    1. Extracts N evenly-spaced frames from the original video
    2. Creates a sampled video from those frames
    3. Runs VIPE SLAM on the sampled video
    4. Extracts trajectory in the correct format
    
    Args:
        video_path: Path to input video
        n_frames: Number of frames to extract (default: 9)
        output_dir: Directory to save VIPE output (optional)
        temp_dir: Directory for temporary sampled video (optional)
        
    Returns:
        tuple: (trajectory, orientations, trajectory_2d) or (None, None, None) if VIPE fails
               - trajectory: numpy array of shape (n_frames, 3) - 3D camera positions in meters
               - orientations: numpy array of shape (n_frames, 3, 3) - rotation matrices
               - trajectory_2d: numpy array of shape (n_frames, 2) - 2D projection (x, y)
    """
    sampled_video_path = None
    
    try:
        # Step 1: Extract N evenly-spaced frames from video
        print(f"  Extracting {n_frames} evenly-spaced frames...")
        frame_paths = extract_n_frames_from_video(video_path, n_frames=n_frames, output_dir=temp_dir)
        
        if not frame_paths or len(frame_paths) == 0:
            print(f"  ✗ Failed to extract frames from video")
            return None, None, None
        
        print(f"  ✓ Extracted {len(frame_paths)} frames")
        
        # Step 2: Create sampled video from extracted frames
        if temp_dir is None:
            temp_dir = tempfile.mkdtemp(prefix="vipe_sampled_")
        
        video_basename = os.path.splitext(os.path.basename(video_path))[0]
        sampled_video_path = os.path.join(temp_dir, f"{video_basename}_sampled.mp4")
        
        print(f"  Creating sampled video with {len(frame_paths)} frames...")
        create_video_from_frames(frame_paths, sampled_video_path, fps=30)
        print(f"  ✓ Created sampled video")
        
        # Step 3: Run VIPE SLAM on the sampled video
        print(f"  Running VIPE SLAM on sampled video...")
        
        # Create video stream (RawMp4Stream expects a Path object)
        video_stream = RawMp4Stream(Path(sampled_video_path))
        
        # Configure VIPE pipeline - use robust settings similar to example_usage.py
        pipeline_cfg = OmegaConf.create({
            'init': {
                'camera_type': 'pinhole',
                'instance': None
            },
            'slam': {
                'buffer': 1024,
                'beta': 0.3,
                'filter_thresh': 1.0,  # Enable motion filtering for robustness
                'warmup': 8,  # More frames for stable initialization
                'keyframe_thresh': 4.0,  # Higher threshold = fewer keyframes, more stable
                'frontend_thresh': 12.0,  # More lenient threshold for sampled frames
                'frontend_window': 20,
                'frontend_radius': 3,  # Larger radius for better connectivity
                'frontend_nms': 1,
                'seq_init': True,
                'frontend_backend_iters': [8, 16, 32],  # Fewer iterations, more stable
                'backend_thresh': 16.0,  # More lenient for sampled frames
                'backend_radius': 3,
                'backend_nms': 2,
                'backend_iters': 12,
                'init_disp': 4.0,  # Lower initial disparity for better convergence
                'optimize_intrinsics': False,
                'optimize_rig_rotation': False,
                'cross_view': False,  # Disable cross-view for simpler optimization
                'cross_view_idx': None,
                'adaptive_cross_view': False,
                'infill_chunk_size': 16,
                'infill_dense_disp': True,  # Enable dense disparity infill
                'map_filter_thresh': 0.1,  # Higher filtering for cleaner map
                'visualize': False,
                'keyframe_depth': 'metric3d-small',
                'n_views': 1,
                'height': 384,
                'width': 512,
                'sparse_tracks': {'name': 'dummy'},
                'ba': {
                    'dense_disp_alpha': 0.01  # Increased regularization for stability
                }
            },
            'post': {
                'depth_align_model': None
            },
            'output': {
                'path': output_dir if output_dir else '/tmp/vipe_output',
                'save_artifacts': False,
                'save_viz': False,
                'save_slam_map': False
            }
        })
        
        # Create and run pipeline
        pipeline = DefaultAnnotationPipeline(
            init=pipeline_cfg.init,
            slam=pipeline_cfg.slam,
            post=pipeline_cfg.post,
            output=pipeline_cfg.output
        )
        
        pipeline.return_payload = True
        result = pipeline.run(video_stream)
        slam_output = result.payload
        
        if slam_output is None:
            print("  Warning: VIPE SLAM returned no output")
            return None, None, None
        
        # Get poses from SLAM trajectory - this is the correct format
        # poses shape: (num_frames, 7) where 7 = [tx, ty, tz, qx, qy, qz, qw]
        poses = slam_output.trajectory.data
        
        if poses is None or poses.shape[0] == 0:
            print("  Warning: No poses extracted from VIPE SLAM")
            return None, None, None
        
        num_poses = poses.shape[0]
        print(f"  ✓ Extracted {num_poses} poses from VIPE SLAM")
        
        # Extract trajectory using the proper conversion function
        # This handles coordinate system conversion (OpenCV to top-down view)
        # and properly interprets the pose format
        trajectory, orientations, trajectory_2d = extract_trajectory_from_poses(poses)
        
        print(f"  ✓ Extracted trajectory with {len(trajectory)} points")
        
        return trajectory, orientations, trajectory_2d
        
    except Exception as e:
        print(f"  ✗ Error running VIPE SLAM: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None
    
    finally:
        # Clean up temporary sampled video and frames
        if sampled_video_path and os.path.exists(sampled_video_path):
            try:
                os.remove(sampled_video_path)
            except:
                pass
        
        # Clean up frame directory if we created it
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
            except:
                pass


# Note: poses_to_trajectory function removed - we now use extract_trajectory_from_poses
# from extrinsic_path.extract_extrinsic_path which properly handles VIPE's pose format
# and coordinate system conversions


def compute_trajectory_metrics(pred_trajectory_2d, gt_trajectory_2d, eval_config):
    """
    Compute trajectory comparison metrics as defined in metrics.md.
    
    Metrics:
    - ADE: Average Displacement Error
    - FDE: Final Displacement Error
    - MR: Miss Rate
    - SE: Soft Endpoint
    - AC: Approach Consistency
    - Overall Score: Weighted combination of all metrics
    
    Args:
        pred_trajectory_2d: Predicted trajectory (N, 2)
        gt_trajectory_2d: Ground truth trajectory (M, 2)
        eval_config: Evaluation configuration dict from YAML
        
    Returns:
        dict: Dictionary of metrics
    """
    # Handle empty trajectories
    if len(pred_trajectory_2d) == 0 or len(gt_trajectory_2d) == 0:
        return {
            'ade': float('inf'),
            'fde': float('inf'),
            'miss_rate': 100.0,
            'se': 0.0,
            'ac': 0.0,
            'overall_score': 0.0,
            'valid': False,
            'pred_points': len(pred_trajectory_2d),
            'gt_points': len(gt_trajectory_2d)
        }
    
    # Extract config parameters
    metrics_config = eval_config['metrics']
    overall_config = eval_config['overall_score']
    
    # Prepare AC parameters
    ac_params = metrics_config['ac']
    
    # Prepare overall score parameters
    overall_score_params = {
        'tau_ade': overall_config['tau_ade'],
        'tau_fde': overall_config['tau_fde'],
        'weights': overall_config['weights']
    }
    
    # Compute all metrics from metrics.py
    metrics = compute_all_metrics(
        pred_trajectory_2d, 
        gt_trajectory_2d,
        miss_threshold=metrics_config['miss_threshold'],
        interpolate_mode=metrics_config['interpolate_mode'],
        se_sigma=metrics_config['se_sigma'],
        ac_params=ac_params,
        overall_score_params=overall_score_params
    )
    
    metrics['valid'] = True
    
    return metrics


def evaluate_world_models(
    segments_dir=None,
    wm_videos_dir=None,
    output_dir=None,
    eval_config_path=None,
    vipe_results_dir=None,
    n_frames=9,
    num_samples=None
):
    """
    Evaluate all world models on all available segments using VIPE.
    
    For each video:
    1. Extracts N evenly-spaced frames from the video
    2. Creates a sampled video from those frames
    3. Runs VIPE SLAM on the sampled video to extract camera trajectory
    4. Compares trajectory with ground truth
    
    Args:
        segments_dir: Directory containing ground truth segments
        wm_videos_dir: Directory containing world model videos
        output_dir: Directory to save evaluation results
        eval_config_path: Path to evaluation metrics configuration file
        vipe_results_dir: Directory containing pre-computed VIPE results (optional)
        n_frames: Number of frames to extract from each video (default: 9)
        num_samples: Number of samples to evaluate (None for all samples)
        
    Returns:
        dict: Complete evaluation results
    """
    # Set default paths
    if segments_dir is None:
        segments_dir = os.path.join(PROJECT_ROOT, "dataset/Target_samples")
    if wm_videos_dir is None:
        wm_videos_dir = os.path.join(PROJECT_ROOT, "dataset/wm_videos")
    if output_dir is None:
        output_dir = os.path.join(PROJECT_ROOT, "evaluation_results/vipe")
    if eval_config_path is None:
        eval_config_path = os.path.join(PROJECT_ROOT, "evaluation/config/vggt_config.yaml")
    if vipe_results_dir is None:
        vipe_results_dir = os.path.join(PROJECT_ROOT, "models/vipe/vipe_results")

    print("="*80)
    print("World Models Evaluation using World Decoder (VIPE)")
    print("="*80)
    
    # Load evaluation configuration
    eval_config = load_evaluation_config(eval_config_path)
    print(f"\nLoaded evaluation config from: {eval_config_path}")
    print(f"  Miss threshold: {eval_config['metrics']['miss_threshold']}m")
    print(f"  Number of frames to extract from each video: {n_frames}")
    print(f"  Frame extraction: Evenly-spaced sampling from original videos")
    
    # Warn if n_frames is too low
    if n_frames < 9:
        print(f"  WARNING: n_frames={n_frames} may be too low for VIPE SLAM stability.")
        print(f"           Recommended: n_frames >= 9 for better results.")
    print(f"  Overall score weights: ADE={eval_config['overall_score']['weights']['ade']}, "
          f"FDE={eval_config['overall_score']['weights']['fde']}, "
          f"MR={eval_config['overall_score']['weights']['mr']}, "
          f"SE={eval_config['overall_score']['weights']['se']}, "
          f"AC={eval_config['overall_score']['weights']['ac']}")
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Create temporary directory for VIPE processing
    temp_dir = os.path.join(output_dir, 'temp_vipe_processing')
    os.makedirs(temp_dir, exist_ok=True)
    
    # Find all segments
    segment_dirs = sorted([d for d in glob.glob(os.path.join(segments_dir, "*/")) if os.path.isdir(d)])
    print(f"\nFound {len(segment_dirs)} segments in total")
    
    # Limit number of samples if specified
    if num_samples is not None and num_samples > 0:
        segment_dirs = segment_dirs[:num_samples]
        print(f"Limiting evaluation to {len(segment_dirs)} samples (--num_samples={num_samples})")
    else:
        print(f"Evaluating all {len(segment_dirs)} segments")
    
    # Find all world model videos
    wm_videos = find_world_model_videos(wm_videos_dir)
    print(f"\nFound {len(wm_videos)} world models: {list(wm_videos.keys())}")
    
    # Initialize results storage
    all_results = {
        'by_model': {},
        'by_segment': {},
        'summary': {}
    }
    
    # Process each model
    for model_name, model_videos in wm_videos.items():
        print(f"\n{'='*80}")
        print(f"Evaluating Model: {model_name}")
        print(f"{'='*80}")
        
        all_results['by_model'][model_name] = {}
        
        # Process each segment for this model
        for segment_dir in tqdm(segment_dirs, desc=f"Processing {model_name}"):
            segment_name = os.path.basename(segment_dir.rstrip('/'))
            
            # Check if we have a video for this segment
            if segment_name not in model_videos:
                print(f"\nWarning: No video found for segment {segment_name} in model {model_name}")
                continue
            
            wm_video_path = model_videos[segment_name]
            
            print(f"\nProcessing: {segment_name}")
            print(f"  WM Video: {os.path.basename(wm_video_path)}")
            
            try:
                # Step 1: Try to load pre-computed VIPE poses first
                precomputed_poses = load_precomputed_vipe_poses(segment_name, vipe_results_dir=vipe_results_dir)
                
                if precomputed_poses is not None:
                    # Convert pre-computed poses to trajectory format
                    # Pre-computed poses are in (N, 7) format: [tx, ty, tz, qx, qy, qz, qw]
                    trajectory, orientations, trajectory_2d = extract_trajectory_from_poses(precomputed_poses)
                    
                    # Sample N evenly-spaced points if we have more than requested
                    total_poses = len(trajectory)
                    if total_poses > n_frames:
                        indices = np.linspace(0, total_poses - 1, n_frames, dtype=int)
                        trajectory = trajectory[indices]
                        orientations = orientations[indices]
                        trajectory_2d = trajectory_2d[indices]
                        print(f"  ✓ Sampled {len(trajectory)} points from {total_poses} total pre-computed poses")
                else:
                    # Step 2: If no pre-computed poses, extract N frames and run VIPE SLAM
                    print(f"  No pre-computed poses found, extracting {n_frames} frames and running VIPE SLAM...")
                    vipe_output_dir = os.path.join(temp_dir, f"{model_name}_{segment_name}_vipe_output")
                    frame_temp_dir = os.path.join(temp_dir, f"{model_name}_{segment_name}_frames")
                    
                    # Run VIPE with frame sampling - this will extract n_frames evenly-spaced frames
                    trajectory, orientations, trajectory_2d = run_vipe_on_video_with_sampling(
                        wm_video_path, 
                        n_frames=n_frames,
                        output_dir=vipe_output_dir,
                        temp_dir=frame_temp_dir
                    )
                    
                    if trajectory is None or len(trajectory) == 0:
                        print(f"  ✗ VIPE failed to extract trajectory")
                        continue
                    
                    print(f"  ✓ Extracted {len(trajectory)} trajectory points from {n_frames} sampled frames")
                
                # Step 4: Compute trajectory length
                trajectory_length_meters = np.sum(np.linalg.norm(np.diff(trajectory, axis=0), axis=1))
                
                print(f"  ✓ Trajectory length: {trajectory_length_meters:.2f}m with {len(trajectory)} points")
                
            except Exception as e:
                print(f"  ✗ Error processing: {e}")
                import traceback
                traceback.print_exc()
                continue
            
            # Load ground truth trajectory
            gt_trajectory_2d = load_ground_truth_trajectory(segment_dir, segment_name)
            
            if gt_trajectory_2d is None:
                print(f"  ✗ No ground truth trajectory found")
                continue
            
            # Compute metrics
            pred_trajectory_2d = trajectory_2d
            metrics = compute_trajectory_metrics(pred_trajectory_2d, gt_trajectory_2d, eval_config)
            
            # Store results
            result_entry = {
                'segment_name': segment_name,
                'model_name': model_name,
                'n_poses': len(trajectory),  # Number of trajectory points
                'trajectory_length': float(trajectory_length_meters),
                'metrics': metrics,
                'trajectory': trajectory.tolist(),
                'trajectory_2d': pred_trajectory_2d.tolist(),
                'gt_trajectory_2d': gt_trajectory_2d.tolist()
            }
            
            all_results['by_model'][model_name][segment_name] = result_entry
            
            # Also store by segment for cross-model comparison
            if segment_name not in all_results['by_segment']:
                all_results['by_segment'][segment_name] = {}
            all_results['by_segment'][segment_name][model_name] = result_entry
            
            print(f"  ✓ Overall: {metrics['overall_score']:.4f} | ADE: {metrics['ade']:.4f}m, FDE: {metrics['fde']:.4f}m, MR: {metrics['miss_rate']:.2f}%, SE: {metrics['se']:.4f}, AC: {metrics['ac']:.4f}")
    
    # Clean up temporary directory
    shutil.rmtree(temp_dir, ignore_errors=True)
    
    # Compute summary statistics
    print(f"\n{'='*80}")
    print("Computing Summary Statistics")
    print(f"{'='*80}")
    
    for model_name, model_results in all_results['by_model'].items():
        if not model_results:
            continue
        
        # Collect all metrics for this model
        ade_values = []
        fde_values = []
        miss_rates = []
        se_values = []
        ac_values = []
        overall_scores = []
        
        for segment_result in model_results.values():
            metrics = segment_result['metrics']
            if metrics['valid']:
                ade_values.append(metrics['ade'])
                fde_values.append(metrics['fde'])
                miss_rates.append(metrics['miss_rate'])
                se_values.append(metrics['se'])
                ac_values.append(metrics['ac'])
                overall_scores.append(metrics['overall_score'])
        
        # Compute summary statistics
        summary = {
            'num_segments': len(model_results),
            'num_valid': len(ade_values),
            'overall_score': {
                'mean': float(np.mean(overall_scores)) if overall_scores else None,
                'median': float(np.median(overall_scores)) if overall_scores else None,
                'std': float(np.std(overall_scores)) if overall_scores else None,
                'min': float(np.min(overall_scores)) if overall_scores else None,
                'max': float(np.max(overall_scores)) if overall_scores else None
            },
            'ade': {
                'mean': float(np.mean(ade_values)) if ade_values else None,
                'median': float(np.median(ade_values)) if ade_values else None,
                'std': float(np.std(ade_values)) if ade_values else None,
                'min': float(np.min(ade_values)) if ade_values else None,
                'max': float(np.max(ade_values)) if ade_values else None
            },
            'fde': {
                'mean': float(np.mean(fde_values)) if fde_values else None,
                'median': float(np.median(fde_values)) if fde_values else None,
                'std': float(np.std(fde_values)) if fde_values else None,
                'min': float(np.min(fde_values)) if fde_values else None,
                'max': float(np.max(fde_values)) if fde_values else None
            },
            'miss_rate': {
                'mean': float(np.mean(miss_rates)) if miss_rates else None,
                'median': float(np.median(miss_rates)) if miss_rates else None,
                'std': float(np.std(miss_rates)) if miss_rates else None,
                'min': float(np.min(miss_rates)) if miss_rates else None,
                'max': float(np.max(miss_rates)) if miss_rates else None
            },
            'se': {
                'mean': float(np.mean(se_values)) if se_values else None,
                'median': float(np.median(se_values)) if se_values else None,
                'std': float(np.std(se_values)) if se_values else None,
                'min': float(np.min(se_values)) if se_values else None,
                'max': float(np.max(se_values)) if se_values else None
            },
            'ac': {
                'mean': float(np.mean(ac_values)) if ac_values else None,
                'median': float(np.median(ac_values)) if ac_values else None,
                'std': float(np.std(ac_values)) if ac_values else None,
                'min': float(np.min(ac_values)) if ac_values else None,
                'max': float(np.max(ac_values)) if ac_values else None
            }
        }
        
        all_results['summary'][model_name] = summary
        
        print(f"\n{model_name}:")
        print(f"  Segments processed: {summary['num_valid']}/{summary['num_segments']}")
        if summary['overall_score']['mean']:
            print(f"  Overall Score: {summary['overall_score']['mean']:.4f} ± {summary['overall_score']['std']:.4f}")
            print(f"  ADE: {summary['ade']['mean']:.4f} ± {summary['ade']['std']:.4f} m")
            print(f"  FDE: {summary['fde']['mean']:.4f} ± {summary['fde']['std']:.4f} m")
            print(f"  Miss Rate (2m): {summary['miss_rate']['mean']:.2f} ± {summary['miss_rate']['std']:.2f} %")
            print(f"  SE (Soft Endpoint): {summary['se']['mean']:.4f} ± {summary['se']['std']:.4f}")
            print(f"  AC (Approach Consistency): {summary['ac']['mean']:.4f} ± {summary['ac']['std']:.4f}")
    
    # Save results to JSON
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    results_json_path = os.path.join(output_dir, f"evaluation_results_{timestamp}.json")
    
    # Convert numpy types for JSON serialization
    def convert_for_json(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.int32, np.int64)):
            return int(obj)
        return obj
    
    with open(results_json_path, 'w') as f:
        json.dump(all_results, f, indent=2, default=convert_for_json)
    
    print(f"\n✓ Saved detailed results to: {results_json_path}")
    
    # Create summary CSV
    create_summary_csv(all_results, output_dir, timestamp)
    
    # Create visualizations
    try:
        create_evaluation_visualizations(all_results, output_dir, timestamp)
    except Exception as e:
        print(f"Warning: Could not create visualizations: {e}")
    
    return all_results


def create_summary_csv(all_results, output_dir, timestamp):
    """Create summary CSV files for easy analysis."""
    
    # 1. Per-segment, per-model results
    segment_model_data = []
    for segment_name, segment_results in all_results['by_segment'].items():
        for model_name, result in segment_results.items():
            metrics = result['metrics']
            row = {
                'segment': segment_name,
                'model': model_name,
                'n_poses': result['n_poses'],
                'trajectory_length': result['trajectory_length'],
                'overall_score': metrics['overall_score'],
                'ade': metrics['ade'],
                'fde': metrics['fde'],
                'miss_rate': metrics['miss_rate'],
                'se': metrics['se'],
                'ac': metrics['ac'],
                'pred_points': metrics['pred_points'],
                'gt_points': metrics['gt_points']
            }
            segment_model_data.append(row)
    
    df_segment_model = pd.DataFrame(segment_model_data)
    csv_path = os.path.join(output_dir, f"segment_model_results_{timestamp}.csv")
    df_segment_model.to_csv(csv_path, index=False)
    print(f"✓ Saved per-segment results to: {csv_path}")
    
    # 2. Model summary statistics
    summary_data = []
    for model_name, summary in all_results['summary'].items():
        row = {
            'model': model_name,
            'num_segments': summary['num_segments'],
            'num_valid': summary['num_valid'],
            'overall_score_mean': summary['overall_score']['mean'],
            'overall_score_std': summary['overall_score']['std'],
            'ade_mean': summary['ade']['mean'],
            'ade_std': summary['ade']['std'],
            'fde_mean': summary['fde']['mean'],
            'fde_std': summary['fde']['std'],
            'miss_rate_mean': summary['miss_rate']['mean'],
            'miss_rate_std': summary['miss_rate']['std'],
            'se_mean': summary['se']['mean'],
            'se_std': summary['se']['std'],
            'ac_mean': summary['ac']['mean'],
            'ac_std': summary['ac']['std']
        }
        summary_data.append(row)
    
    df_summary = pd.DataFrame(summary_data)
    csv_path = os.path.join(output_dir, f"model_summary_{timestamp}.csv")
    df_summary.to_csv(csv_path, index=False)
    print(f"✓ Saved model summary to: {csv_path}")


def create_evaluation_visualizations(all_results, output_dir, timestamp):
    """Create comprehensive visualizations of evaluation results."""
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    
    # Sort models: gt_video first, then others alphabetically
    models = list(all_results['summary'].keys())
    models_sorted = sorted(models, key=lambda x: (x != 'gt_video', x))
    
    # Create consistent color mapping for all models
    # Use tab20 for more distinct colors
    color_palette = cm.tab20(np.linspace(0, 1, 20))
    model_colors = {}
    for i, model_name in enumerate(models_sorted):
        model_colors[model_name] = color_palette[i % 20]
    
    # 1. Model comparison - bar charts (using metrics from metrics.md + overall score)
    fig, axes = plt.subplots(2, 3, figsize=(22, 16))
    
    metrics_to_plot = ['overall_score', 'ade', 'fde', 'miss_rate', 'se', 'ac']
    titles = ['Overall Score', 'ADE (m)', 'FDE (m)', 'Miss Rate (%)', 'SE', 'AC']
    
    for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):
        ax = axes[idx // 3, idx % 3]
        
        # Filter valid models and sort (gt_video first)
        valid_models = [m for m in models_sorted if all_results['summary'][m][metric]['mean'] is not None]
        means = [all_results['summary'][m][metric]['mean'] for m in valid_models]
        
        if means:
            x_pos = np.arange(len(valid_models))
            # Get colors for each model
            colors = [model_colors[m] for m in valid_models]
            
            # Plot without error bars
            ax.bar(x_pos, means, alpha=0.7, color=colors)
            ax.set_xticks(x_pos + 0.4)  # Shift ticks slightly right
            ax.set_xticklabels(valid_models, rotation=75, ha='right', fontsize=30)  # 2x bigger
            # 2x bigger ax.set_ylabel(title, fontsize=30)  
            ax.set_title(f'{title}', 
                        fontweight='bold' if metric == 'overall_score' else 'normal',
                        fontsize=36)  # 2x bigger
            ax.tick_params(axis='y', labelsize=26)  # 2x bigger for y-axis ticks
            ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.subplots_adjust(wspace=0.15) # Add horizontal space between subplots
    plt.savefig(os.path.join(output_dir, f"model_comparison_{timestamp}.png"), dpi=300, bbox_inches='tight')
    print(f"✓ Saved model comparison plot")
    plt.close()
    
    # 2. Trajectory visualizations for each segment
    for segment_name, segment_results in all_results['by_segment'].items():
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # Plot GT trajectory
        first_result = list(segment_results.values())[0]
        gt_traj = np.array(first_result['gt_trajectory_2d'])
        ax.plot(gt_traj[:, 0], gt_traj[:, 1], 'k--', linewidth=3, label='Ground Truth', zorder=10)
        ax.scatter(gt_traj[0, 0], gt_traj[0, 1], c='green', s=200, marker='o', edgecolors='black', linewidth=2, label='GT Start', zorder=11)
        ax.scatter(gt_traj[-1, 0], gt_traj[-1, 1], c='red', s=200, marker='s', edgecolors='black', linewidth=2, label='GT End', zorder=11)
        
        # Collect all x values to adjust xlim and reduce left padding
        all_x_values = gt_traj[:, 0].tolist()
        
        # Plot predicted trajectories for each model (using consistent colors)
        for model_name, result in segment_results.items():
            pred_traj = np.array(result['trajectory_2d'])
            color = model_colors[model_name]  # Use consistent color from model_colors
            
            ax.plot(pred_traj[:, 0], pred_traj[:, 1], '-', color=color, linewidth=2, 
                   alpha=0.7, label=f'{model_name}')
            ax.scatter(pred_traj[0, 0], pred_traj[0, 1], c=color, s=80, marker='o', zorder=5)
            ax.scatter(pred_traj[-1, 0], pred_traj[-1, 1], c=color, s=80, marker='s', zorder=5)
            
            # Collect x values
            all_x_values.extend(pred_traj[:, 0].tolist())
        
        # Set xlim to reduce left empty space
        min_x = min(all_x_values)
        max_x = max(all_x_values)
        x_range = max_x - min_x
        ax.set_xlim(min_x - 0.01 * x_range, max_x + 0.05 * x_range)  # Minimal left margin
        
        ax.set_xlabel('X (meters)', fontsize=30)
        ax.set_ylabel('Y (meters)', fontsize=30)
        ax.set_title(f'Trajectory Comparison\n{segment_name}', fontsize=36, pad=20)  # Two rows, more padding
        ax.tick_params(axis='both', labelsize=26)
        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=24)  # Legend outside on right
        ax.axis('equal')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"trajectory_{segment_name}_{timestamp}.png"), dpi=300, bbox_inches='tight')
        plt.close()
    
    print(f"✓ Saved {len(all_results['by_segment'])} trajectory comparison plots")


def main():
    """Main evaluation function."""
    parser = argparse.ArgumentParser(
        description='Evaluate World Models Performance using World Decoder (VIPE Version)'
    )
    parser.add_argument(
        '-n', '--num_samples',
        type=int,
        default=None,
        help='Number of samples to evaluate (default: all samples)'
    )
    parser.add_argument(
        '--segments_dir',
        type=str,
        default=None,
        help='Directory containing ground truth segments'
    )
    parser.add_argument(
        '--wm_videos_dir',
        type=str,
        default=None,
        help='Directory containing world model videos'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default=None,
        help='Directory to save evaluation results'
    )
    parser.add_argument(
        '--eval_config_path',
        type=str,
        default=None,
        help='Path to evaluation metrics configuration file'
    )
    parser.add_argument(
        '--vipe_results_dir',
        type=str,
        default=None,
        help='Directory containing pre-computed VIPE results'
    )
    parser.add_argument(
        '--n_frames',
        type=int,
        default=9,
        help='Number of frames to extract from each video (default: 9)'
    )
    
    args = parser.parse_args()
    
    # Run evaluation
    results = evaluate_world_models(
        segments_dir=args.segments_dir,
        wm_videos_dir=args.wm_videos_dir,
        output_dir=args.output_dir,
        eval_config_path=args.eval_config_path,
        vipe_results_dir=args.vipe_results_dir,
        n_frames=args.n_frames,
        num_samples=args.num_samples
    )
    
    print("\n" + "="*80)
    print("Evaluation Complete!")
    print("="*80)
    output_dir_to_show = args.output_dir if args.output_dir else os.path.join(PROJECT_ROOT, "evaluation_results/vipe")
    print(f"Results saved to: {output_dir_to_show}")
    print("\nTo view results:")
    print(f"  - Summary CSV: {output_dir_to_show}/model_summary_*.csv")
    print(f"  - Detailed results: {output_dir_to_show}/segment_model_results_*.csv")
    print(f"  - Full JSON: {output_dir_to_show}/evaluation_results_*.json")
    print(f"  - Visualizations: {output_dir_to_show}/*.png")


if __name__ == "__main__":
    main()