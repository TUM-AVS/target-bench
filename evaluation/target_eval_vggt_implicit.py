#!/usr/bin/env python3
"""
Evaluate World Models Performance using World Decoder

This script:
1. Processes videos generated by world models (veo3.1, veo3.1fast, sora2, etc.)
2. Extracts trajectories using the VGGT world decoder
3. Compares extracted trajectories with ground truth
4. Computes evaluation metrics (RMSE, ATE, RPE, etc.)
5. Generates comparison visualizations and reports

Directory structure expected:
- dataset/Target_samples/{segment_name}/ - Ground truth data
  - {segment_name}_first_frame.png
  - {segment_name}_current_frame.png
  - {segment_name}_metadata.json
  - {segment_name}_trajectory.csv (ground truth trajectory)
  
- dataset/wm_videos/{model_name}/ - World model generated videos
  - {segment_name}.mp4
"""

import os
import sys
import glob
import json
import yaml
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm.auto import tqdm
import torch
import cv2
import argparse

# Define project root and add paths
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# Add paths
sys.path.append(os.path.join(PROJECT_ROOT, 'pipelines/vggt'))
sys.path.append(os.path.join(PROJECT_ROOT, 'models/vggt'))

from extrinsic_path.extract_extrinsic_path import (
    extract_trajectory_from_extrinsics,
    load_ground_truth_trajectory
)
from scale_factor.compute_scale_factor import (
    setup_model,
    load_config,
    extract_frames
)
from vggt.utils.load_fn import load_and_preprocess_images
from vggt.utils.pose_enc import pose_encoding_to_extri_intri

# Import trajectory metrics
sys.path.append(os.path.join(PROJECT_ROOT, 'evaluation'))
from metrics import compute_all_metrics


def load_evaluation_config(config_path=None):
    """
    Load evaluation configuration from YAML file.
    
    Args:
        config_path: Path to config YAML file
        
    Returns:
        dict: Configuration dictionary
    """
    if config_path is None:
        config_path = os.path.join(PROJECT_ROOT, "evaluation/config/vggt_config.yaml")
        
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def load_explicit_implicit_mapping(csv_path=None):
    """
    Load explicit/implicit mapping from CSV file.
    
    Args:
        csv_path: Path to CSV file with columns: name, is_explicit
        
    Returns:
        dict: {segment_name: is_explicit (1=explicit, 0=implicit)}
    """
    if csv_path is None:
        csv_path = os.path.join(PROJECT_ROOT, "dataset/im_ex_test_only.csv")
        
    df = pd.read_csv(csv_path)
    mapping = {}
    for _, row in df.iterrows():
        mapping[row['name']] = int(row['is_explicit'])
    print(f"\nLoaded explicit/implicit mapping from: {csv_path}")
    print(f"  Total samples: {len(mapping)}")
    print(f"  Explicit samples: {sum(1 for v in mapping.values() if v == 1)}")
    print(f"  Implicit samples: {sum(1 for v in mapping.values() if v == 0)}")
    return mapping


def find_world_model_videos(wm_videos_dir=None):
    """
    Find all world model videos organized by model type.
    
    Returns:
        dict: {model_name: {segment_name: video_path}}
    """
    if wm_videos_dir is None:
        wm_videos_dir = os.path.join(PROJECT_ROOT, "dataset/wm_videos")
        
    wm_videos = {}
    
    # Find all subdirectories in wm_videos (each is a model)
    if not os.path.exists(wm_videos_dir):
        print(f"Warning: World model videos directory not found: {wm_videos_dir}")
        return {}
        
    model_dirs = [d for d in glob.glob(os.path.join(wm_videos_dir, "*/")) if os.path.isdir(d)]
    
    for model_dir in model_dirs:
        model_name = os.path.basename(model_dir.rstrip('/'))
        wm_videos[model_name] = {}
        
        # Find all mp4 files in this model directory
        video_files = glob.glob(os.path.join(model_dir, "*.mp4"))
        
        for video_path in video_files:
            # Extract segment name from filename (remove .mp4 extension)
            segment_name = os.path.splitext(os.path.basename(video_path))[0]
            wm_videos[model_name][segment_name] = video_path
        
        print(f"Found {len(wm_videos[model_name])} videos for model: {model_name}")
    
    return wm_videos


# NOTE: process_wm_video_for_segment function has been removed and inlined into evaluate_world_models
# to avoid reloading the VGGT model on every iteration. The model is now loaded once before the loop.


def compute_trajectory_metrics(pred_trajectory_2d, gt_trajectory_2d, eval_config):
    """
    Compute trajectory comparison metrics as defined in metrics.md.
    
    Metrics:
    - ADE: Average Displacement Error
    - FDE: Final Displacement Error
    - MR: Miss Rate
    - SE: Soft Endpoint
    - AC: Approach Consistency
    - Overall Score: Weighted combination of all metrics
    
    Args:
        pred_trajectory_2d: Predicted trajectory (N, 2)
        gt_trajectory_2d: Ground truth trajectory (M, 2)
        eval_config: Evaluation configuration dict from YAML
        
    Returns:
        dict: Dictionary of metrics
    """
    # Handle empty trajectories
    if len(pred_trajectory_2d) == 0 or len(gt_trajectory_2d) == 0:
        return {
            'ade': float('inf'),
            'fde': float('inf'),
            'miss_rate': 100.0,
            'se': 0.0,
            'ac': 0.0,
            'overall_score': 0.0,
            'valid': False,
            'pred_points': len(pred_trajectory_2d),
            'gt_points': len(gt_trajectory_2d)
        }
    
    # Extract config parameters
    metrics_config = eval_config['metrics']
    overall_config = eval_config['overall_score']
    
    # Prepare AC parameters
    ac_params = metrics_config['ac']
    
    # Prepare overall score parameters
    overall_score_params = {
        'tau_ade': overall_config['tau_ade'],
        'tau_fde': overall_config['tau_fde'],
        'weights': overall_config['weights']
    }
    
    # Compute all metrics from metrics.py
    metrics = compute_all_metrics(
        pred_trajectory_2d, 
        gt_trajectory_2d,
        miss_threshold=metrics_config['miss_threshold'],
        interpolate_mode=metrics_config['interpolate_mode'],
        se_sigma=metrics_config['se_sigma'],
        ac_params=ac_params,
        overall_score_params=overall_score_params
    )
    
    metrics['valid'] = True
    
    return metrics


def evaluate_world_models(
    segments_dir=None,
    wm_videos_dir=None,
    output_dir=None,
    config_path=None,
    eval_config_path=None,
    explicit_implicit_csv=None,
    num_samples=None
):
    """
    Evaluate all world models on all available segments, separated by explicit/implicit.
    
    Args:
        segments_dir: Directory containing ground truth segments
        wm_videos_dir: Directory containing world model videos
        output_dir: Directory to save evaluation results
        config_path: Path to VGGT configuration file
        eval_config_path: Path to evaluation metrics configuration file
        explicit_implicit_csv: Path to CSV file with explicit/implicit labels
        num_samples: Number of samples to evaluate (None for all samples)
        
    Returns:
        dict: Complete evaluation results
    """
    # Set default paths
    if segments_dir is None:
        segments_dir = os.path.join(PROJECT_ROOT, "dataset/Target_samples")
    if wm_videos_dir is None:
        wm_videos_dir = os.path.join(PROJECT_ROOT, "dataset/wm_videos")
    if output_dir is None:
        output_dir = os.path.join(PROJECT_ROOT, "evaluation_results")
    if config_path is None:
        config_path = os.path.join(PROJECT_ROOT, "pipelines/vggt/configs/vggt_pcd_configs.yml")
    if eval_config_path is None:
        eval_config_path = os.path.join(PROJECT_ROOT, "evaluation/config/vggt_config.yaml")
    if explicit_implicit_csv is None:
        explicit_implicit_csv = os.path.join(PROJECT_ROOT, "dataset/im_ex_test_only.csv")

    print("="*80)
    print("World Models Evaluation using World Decoder")
    print("="*80)
    
    # Load evaluation configuration
    eval_config = load_evaluation_config(eval_config_path)
    print(f"\nLoaded evaluation config from: {eval_config_path}")
    print(f"  Miss threshold: {eval_config['metrics']['miss_threshold']}m")
    print(f"  Overall score weights: ADE={eval_config['overall_score']['weights']['ade']}, "
          f"FDE={eval_config['overall_score']['weights']['fde']}, "
          f"MR={eval_config['overall_score']['weights']['mr']}, "
          f"SE={eval_config['overall_score']['weights']['se']}, "
          f"AC={eval_config['overall_score']['weights']['ac']}")
    
    # Load explicit/implicit mapping
    explicit_implicit_map = load_explicit_implicit_mapping(explicit_implicit_csv)
    
    # Load VGGT configuration
    vggt_config = load_config(config_path)
    
    # Setup VGGT model ONCE before processing all videos
    print("\n" + "="*80)
    print("Loading VGGT Model (one-time setup)")
    print("="*80)
    model, device, dtype = setup_model()
    print(f"✓ Model loaded successfully on {device} with dtype {dtype}")
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Find all segments
    segment_dirs = sorted([d for d in glob.glob(os.path.join(segments_dir, "*/")) if os.path.isdir(d)])
    print(f"\nFound {len(segment_dirs)} segments in total")
    
    # Limit number of samples if specified
    if num_samples is not None and num_samples > 0:
        segment_dirs = segment_dirs[:num_samples]
        print(f"Limiting evaluation to {len(segment_dirs)} samples (--num_samples={num_samples})")
    else:
        print(f"Evaluating all {len(segment_dirs)} segments")
    
    # Find all world model videos
    wm_videos = find_world_model_videos(wm_videos_dir)
    print(f"\nFound {len(wm_videos)} world models: {list(wm_videos.keys())}")
    
    # Initialize results storage
    all_results = {
        'by_model': {},
        'by_segment': {},
        'summary': {},
        'summary_explicit': {},
        'summary_implicit': {},
        'explicit_implicit_map': explicit_implicit_map
    }
    
    # Process each model
    for model_name, model_videos in wm_videos.items():
        print(f"\n{'='*80}")
        print(f"Evaluating Model: {model_name}")
        print(f"{'='*80}")
        
        all_results['by_model'][model_name] = {}
        
        # Process each segment for this model
        for segment_dir in tqdm(segment_dirs, desc=f"Processing {model_name}"):
            segment_name = os.path.basename(segment_dir.rstrip('/'))
            
            # Check if we have a video for this segment
            if segment_name not in model_videos:
                print(f"Warning: No video found for segment {segment_name} in model {model_name}")
                continue
            
            wm_video_path = model_videos[segment_name]
            
            print(f"\nProcessing: {segment_name}")
            print(f"  WM Video: {os.path.basename(wm_video_path)}")
            
            # === INLINE PROCESSING LOGIC (using pre-loaded model) ===
            try:
                # Construct file paths
                current_frame_path = os.path.join(segment_dir, f"{segment_name}_current_frame.png")
                first_frame_path = os.path.join(segment_dir, f"{segment_name}_first_frame.png")
                metadata_path = os.path.join(segment_dir, f"{segment_name}_metadata.json")
                
                # Check if required files exist
                if not all(os.path.exists(path) for path in [current_frame_path, first_frame_path, metadata_path]):
                    print(f"  ✗ Missing required files")
                    continue
                
                # Check if video exists
                if not os.path.exists(wm_video_path):
                    print(f"  ✗ Video path does not exist: {wm_video_path}")
                    continue
                
                # Load metadata to get real displacement
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                
                real_displacement = metadata["distance_first_to_current"]["distance_meters"]
                
                # Setup camera paths: first frame + predicted frames from world model video
                camera_paths = []
                
                # Extract frames from world model generated video
                # Use temp directory for video frames
                temp_frame_dir = os.path.join(output_dir, 'video_frames')
                os.makedirs(temp_frame_dir, exist_ok=True)
                predicted_image_paths = extract_frames(wm_video_path, temp_frame_dir)
                
                camera_paths.extend(predicted_image_paths)
                camera_paths.append(first_frame_path)
                
                # Load and preprocess images
                images = load_and_preprocess_images(camera_paths).to(device)
                
                # Run VGGT inference with pre-loaded model
                with torch.no_grad():
                    with torch.cuda.amp.autocast(dtype=dtype):
                        predictions = model(images)
                    
                        images = images[None]  # add batch dimension
                        aggregated_tokens_list, ps_idx = model.aggregator(images)
                                
                    # Predict Cameras
                    pose_enc = model.camera_head(aggregated_tokens_list)[-1]
                    # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)
                    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])
                    
                    # Extract camera positions from extrinsic matrices
                    # First camera: reference frame (first_frame.png)
                    # Second camera: first predicted frame from world model
                    camera1_position = extrinsic[0, 0, :3, 3]  # Reference frame position
                    camera2_position = extrinsic[0, -1, :3, 3]  # First predicted frame position
                    
                    # Calculate Euclidean distance between camera positions
                    camera_displacement = torch.norm(camera1_position - camera2_position).item()
                
                # Compute scale factor
                if camera_displacement == 0:
                    print(f"  ✗ Zero camera displacement")
                    continue
                else:
                    scale_factor = real_displacement / camera_displacement
                
                # Extract trajectory from extrinsic matrices
                trajectory, orientations, trajectory_2d = extract_trajectory_from_extrinsics(
                    extrinsic.cpu().numpy(), 
                    scale_factor
                )
                
                trajectory_length_meters = np.sum(np.linalg.norm(np.diff(trajectory, axis=0), axis=1))
                
            except Exception as e:
                print(f"  ✗ Error processing: {e}")
                import traceback
                traceback.print_exc()
                continue
            # === END INLINE PROCESSING ===
            
            # Load ground truth trajectory
            gt_trajectory_2d = load_ground_truth_trajectory(segment_dir, segment_name)
            
            if gt_trajectory_2d is None:
                print(f"  ✗ No ground truth trajectory found")
                continue
            
            # Compute metrics
            pred_trajectory_2d = trajectory_2d
            metrics = compute_trajectory_metrics(pred_trajectory_2d, gt_trajectory_2d, eval_config)
            
            # Get category (explicit/implicit)
            is_explicit = explicit_implicit_map.get(segment_name, None)
            category = 'explicit' if is_explicit == 1 else 'implicit' if is_explicit == 0 else 'unknown'
            
            # Store results
            result_entry = {
                'segment_name': segment_name,
                'model_name': model_name,
                'is_explicit': is_explicit,
                'category': category,
                'scale_factor': float(scale_factor),
                'real_displacement': float(real_displacement),
                'camera_displacement': float(camera_displacement),
                'trajectory_length': float(trajectory_length_meters),
                'metrics': metrics,
                'trajectory': trajectory.tolist(),
                'trajectory_2d': pred_trajectory_2d.tolist(),
                'gt_trajectory_2d': gt_trajectory_2d.tolist()
            }
            
            all_results['by_model'][model_name][segment_name] = result_entry
            
            # Also store by segment for cross-model comparison
            if segment_name not in all_results['by_segment']:
                all_results['by_segment'][segment_name] = {}
            all_results['by_segment'][segment_name][model_name] = result_entry
            
            print(f"  ✓ Overall: {metrics['overall_score']:.4f} | ADE: {metrics['ade']:.4f}m, FDE: {metrics['fde']:.4f}m, MR: {metrics['miss_rate']:.2f}%, SE: {metrics['se']:.4f}, AC: {metrics['ac']:.4f}")
    
    # Compute summary statistics
    print(f"\n{'='*80}")
    print("Computing Summary Statistics")
    print(f"{'='*80}")
    
    def compute_summary_for_category(model_results, category_filter=None):
        """
        Compute summary statistics for a specific category.
        
        Args:
            model_results: Dictionary of model results
            category_filter: 'explicit', 'implicit', or None (all)
        """
        ade_values = []
        fde_values = []
        miss_rates = []
        se_values = []
        ac_values = []
        overall_scores = []
        
        for segment_result in model_results.values():
            # Filter by category if specified
            if category_filter is not None:
                if segment_result.get('category') != category_filter:
                    continue
            
            metrics = segment_result['metrics']
            if metrics['valid']:
                ade_values.append(metrics['ade'])
                fde_values.append(metrics['fde'])
                miss_rates.append(metrics['miss_rate'])
                se_values.append(metrics['se'])
                ac_values.append(metrics['ac'])
                overall_scores.append(metrics['overall_score'])
        
        return {
            'num_valid': len(ade_values),
            'overall_score': {
                'mean': float(np.mean(overall_scores)) if overall_scores else None,
                'median': float(np.median(overall_scores)) if overall_scores else None,
                'std': float(np.std(overall_scores)) if overall_scores else None,
                'min': float(np.min(overall_scores)) if overall_scores else None,
                'max': float(np.max(overall_scores)) if overall_scores else None
            },
            'ade': {
                'mean': float(np.mean(ade_values)) if ade_values else None,
                'median': float(np.median(ade_values)) if ade_values else None,
                'std': float(np.std(ade_values)) if ade_values else None,
                'min': float(np.min(ade_values)) if ade_values else None,
                'max': float(np.max(ade_values)) if ade_values else None
            },
            'fde': {
                'mean': float(np.mean(fde_values)) if fde_values else None,
                'median': float(np.median(fde_values)) if fde_values else None,
                'std': float(np.std(fde_values)) if fde_values else None,
                'min': float(np.min(fde_values)) if fde_values else None,
                'max': float(np.max(fde_values)) if fde_values else None
            },
            'miss_rate': {
                'mean': float(np.mean(miss_rates)) if miss_rates else None,
                'median': float(np.median(miss_rates)) if miss_rates else None,
                'std': float(np.std(miss_rates)) if miss_rates else None,
                'min': float(np.min(miss_rates)) if miss_rates else None,
                'max': float(np.max(miss_rates)) if miss_rates else None
            },
            'se': {
                'mean': float(np.mean(se_values)) if se_values else None,
                'median': float(np.median(se_values)) if se_values else None,
                'std': float(np.std(se_values)) if se_values else None,
                'min': float(np.min(se_values)) if se_values else None,
                'max': float(np.max(se_values)) if se_values else None
            },
            'ac': {
                'mean': float(np.mean(ac_values)) if ac_values else None,
                'median': float(np.median(ac_values)) if ac_values else None,
                'std': float(np.std(ac_values)) if ac_values else None,
                'min': float(np.min(ac_values)) if ac_values else None,
                'max': float(np.max(ac_values)) if ac_values else None
            }
        }
    
    for model_name, model_results in all_results['by_model'].items():
        if not model_results:
            continue
        
        # Overall summary
        summary = compute_summary_for_category(model_results, category_filter=None)
        summary['num_segments'] = len(model_results)
        all_results['summary'][model_name] = summary
        
        # Explicit summary
        summary_explicit = compute_summary_for_category(model_results, category_filter='explicit')
        all_results['summary_explicit'][model_name] = summary_explicit
        
        # Implicit summary
        summary_implicit = compute_summary_for_category(model_results, category_filter='implicit')
        all_results['summary_implicit'][model_name] = summary_implicit
        
        # Print overall summary
        print(f"\n{model_name} (Overall):")
        print(f"  Segments processed: {summary['num_valid']}/{summary['num_segments']}")
        if summary['overall_score']['mean']:
            print(f"  Overall Score: {summary['overall_score']['mean']:.4f} ± {summary['overall_score']['std']:.4f}")
            print(f"  ADE: {summary['ade']['mean']:.4f} ± {summary['ade']['std']:.4f} m")
            print(f"  FDE: {summary['fde']['mean']:.4f} ± {summary['fde']['std']:.4f} m")
            print(f"  Miss Rate (2m): {summary['miss_rate']['mean']:.2f} ± {summary['miss_rate']['std']:.2f} %")
            print(f"  SE (Soft Endpoint): {summary['se']['mean']:.4f} ± {summary['se']['std']:.4f}")
            print(f"  AC (Approach Consistency): {summary['ac']['mean']:.4f} ± {summary['ac']['std']:.4f}")
        
        # Print explicit summary
        print(f"\n{model_name} (Explicit):")
        print(f"  Segments processed: {summary_explicit['num_valid']}")
        if summary_explicit['overall_score']['mean']:
            print(f"  Overall Score: {summary_explicit['overall_score']['mean']:.4f} ± {summary_explicit['overall_score']['std']:.4f}")
            print(f"  ADE: {summary_explicit['ade']['mean']:.4f} ± {summary_explicit['ade']['std']:.4f} m")
            print(f"  FDE: {summary_explicit['fde']['mean']:.4f} ± {summary_explicit['fde']['std']:.4f} m")
            print(f"  Miss Rate (2m): {summary_explicit['miss_rate']['mean']:.2f} ± {summary_explicit['miss_rate']['std']:.2f} %")
            print(f"  SE (Soft Endpoint): {summary_explicit['se']['mean']:.4f} ± {summary_explicit['se']['std']:.4f}")
            print(f"  AC (Approach Consistency): {summary_explicit['ac']['mean']:.4f} ± {summary_explicit['ac']['std']:.4f}")
        
        # Print implicit summary
        print(f"\n{model_name} (Implicit):")
        print(f"  Segments processed: {summary_implicit['num_valid']}")
        if summary_implicit['overall_score']['mean']:
            print(f"  Overall Score: {summary_implicit['overall_score']['mean']:.4f} ± {summary_implicit['overall_score']['std']:.4f}")
            print(f"  ADE: {summary_implicit['ade']['mean']:.4f} ± {summary_implicit['ade']['std']:.4f} m")
            print(f"  FDE: {summary_implicit['fde']['mean']:.4f} ± {summary_implicit['fde']['std']:.4f} m")
            print(f"  Miss Rate (2m): {summary_implicit['miss_rate']['mean']:.2f} ± {summary_implicit['miss_rate']['std']:.2f} %")
            print(f"  SE (Soft Endpoint): {summary_implicit['se']['mean']:.4f} ± {summary_implicit['se']['std']:.4f}")
            print(f"  AC (Approach Consistency): {summary_implicit['ac']['mean']:.4f} ± {summary_implicit['ac']['std']:.4f}")
    
    # Save results to JSON
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    results_json_path = os.path.join(output_dir, f"evaluation_results_{timestamp}.json")
    
    # Convert numpy types for JSON serialization
    def convert_for_json(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.int32, np.int64)):
            return int(obj)
        return obj
    
    with open(results_json_path, 'w') as f:
        json.dump(all_results, f, indent=2, default=convert_for_json)
    
    print(f"\n✓ Saved detailed results to: {results_json_path}")
    
    # Create summary CSV
    create_summary_csv(all_results, output_dir, timestamp)
    
    # Create visualizations
    try:
        create_evaluation_visualizations(all_results, output_dir, timestamp)
    except Exception as e:
        print(f"Warning: Could not create visualizations: {e}")
    
    return all_results


def create_summary_csv(all_results, output_dir, timestamp):
    """Create summary CSV files for easy analysis."""
    
    # 1. Per-segment, per-model results
    segment_model_data = []
    for segment_name, segment_results in all_results['by_segment'].items():
        for model_name, result in segment_results.items():
            metrics = result['metrics']
            row = {
                'segment': segment_name,
                'model': model_name,
                'category': result.get('category', 'unknown'),
                'is_explicit': result.get('is_explicit', None),
                'scale_factor': result['scale_factor'],
                'real_displacement': result['real_displacement'],
                'trajectory_length': result['trajectory_length'],
                'overall_score': metrics['overall_score'],
                'ade': metrics['ade'],
                'fde': metrics['fde'],
                'miss_rate': metrics['miss_rate'],
                'se': metrics['se'],
                'ac': metrics['ac'],
                'pred_points': metrics['pred_points'],
                'gt_points': metrics['gt_points']
            }
            segment_model_data.append(row)
    
    df_segment_model = pd.DataFrame(segment_model_data)
    csv_path = os.path.join(output_dir, f"segment_model_results_{timestamp}.csv")
    df_segment_model.to_csv(csv_path, index=False)
    print(f"✓ Saved per-segment results to: {csv_path}")
    
    # 2. Model summary statistics (overall)
    summary_data = []
    for model_name, summary in all_results['summary'].items():
        row = {
            'model': model_name,
            'category': 'overall',
            'num_segments': summary['num_segments'],
            'num_valid': summary['num_valid'],
            'overall_score_mean': summary['overall_score']['mean'],
            'overall_score_std': summary['overall_score']['std'],
            'ade_mean': summary['ade']['mean'],
            'ade_std': summary['ade']['std'],
            'fde_mean': summary['fde']['mean'],
            'fde_std': summary['fde']['std'],
            'miss_rate_mean': summary['miss_rate']['mean'],
            'miss_rate_std': summary['miss_rate']['std'],
            'se_mean': summary['se']['mean'],
            'se_std': summary['se']['std'],
            'ac_mean': summary['ac']['mean'],
            'ac_std': summary['ac']['std']
        }
        summary_data.append(row)
    
    # 3. Model summary statistics (explicit)
    for model_name, summary in all_results['summary_explicit'].items():
        row = {
            'model': model_name,
            'category': 'explicit',
            'num_segments': None,
            'num_valid': summary['num_valid'],
            'overall_score_mean': summary['overall_score']['mean'],
            'overall_score_std': summary['overall_score']['std'],
            'ade_mean': summary['ade']['mean'],
            'ade_std': summary['ade']['std'],
            'fde_mean': summary['fde']['mean'],
            'fde_std': summary['fde']['std'],
            'miss_rate_mean': summary['miss_rate']['mean'],
            'miss_rate_std': summary['miss_rate']['std'],
            'se_mean': summary['se']['mean'],
            'se_std': summary['se']['std'],
            'ac_mean': summary['ac']['mean'],
            'ac_std': summary['ac']['std']
        }
        summary_data.append(row)
    
    # 4. Model summary statistics (implicit)
    for model_name, summary in all_results['summary_implicit'].items():
        row = {
            'model': model_name,
            'category': 'implicit',
            'num_segments': None,
            'num_valid': summary['num_valid'],
            'overall_score_mean': summary['overall_score']['mean'],
            'overall_score_std': summary['overall_score']['std'],
            'ade_mean': summary['ade']['mean'],
            'ade_std': summary['ade']['std'],
            'fde_mean': summary['fde']['mean'],
            'fde_std': summary['fde']['std'],
            'miss_rate_mean': summary['miss_rate']['mean'],
            'miss_rate_std': summary['miss_rate']['std'],
            'se_mean': summary['se']['mean'],
            'se_std': summary['se']['std'],
            'ac_mean': summary['ac']['mean'],
            'ac_std': summary['ac']['std']
        }
        summary_data.append(row)
    
    df_summary = pd.DataFrame(summary_data)
    csv_path = os.path.join(output_dir, f"model_summary_{timestamp}.csv")
    df_summary.to_csv(csv_path, index=False)
    print(f"✓ Saved model summary to: {csv_path}")


def create_evaluation_visualizations(all_results, output_dir, timestamp):
    """Create comprehensive visualizations of evaluation results."""
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    
    # Sort models: gt_video first, then others alphabetically
    models = list(all_results['summary'].keys())
    models_sorted = sorted(models, key=lambda x: (x != 'gt_video', x))
    
    # Create consistent color mapping for all models
    # Use tab20 for more distinct colors
    color_palette = cm.tab20(np.linspace(0, 1, 20))
    model_colors = {}
    for i, model_name in enumerate(models_sorted):
        model_colors[model_name] = color_palette[i % 20]
    
    # 1. Model comparison - bar charts (Overall)
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    
    metrics_to_plot = ['overall_score', 'ade', 'fde', 'miss_rate', 'se', 'ac']
    titles = ['Weighted Overall Score', 'ADE (m)', 'FDE (m)', 'Miss Rate (%)', 'SE (Soft Endpoint)', 'AC (Approach Consistency)']
    
    for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):
        ax = axes[idx // 3, idx % 3]
        
        # Filter valid models and sort (gt_video first)
        valid_models = [m for m in models_sorted if all_results['summary'][m][metric]['mean'] is not None]
        means = [all_results['summary'][m][metric]['mean'] for m in valid_models]
        
        if means:
            x_pos = np.arange(len(valid_models))
            # Get colors for each model
            colors = [model_colors[m] for m in valid_models]
            
            # Plot without error bars
            ax.bar(x_pos, means, alpha=0.7, color=colors)
            ax.set_xticks(x_pos)
            ax.set_xticklabels(valid_models, rotation=45, ha='right', fontsize=30)  # 2x bigger
            ax.set_ylabel(title, fontsize=30)  # 2x bigger
            ax.set_title(f'{title} (Overall)', 
                        fontweight='bold' if metric == 'overall_score' else 'normal',
                        fontsize=36)  # 2x bigger
            ax.tick_params(axis='y', labelsize=26)  # 2x bigger for y-axis ticks
            ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.subplots_adjust(wspace=0.15) # Add horizontal space between subplots
    plt.savefig(os.path.join(output_dir, f"model_comparison_overall_{timestamp}.png"), dpi=300, bbox_inches='tight')
    print(f"✓ Saved overall model comparison plot")
    plt.close()
    
    # 2. Model comparison - Explicit vs Implicit side by side
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    
    for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):
        ax = axes[idx // 3, idx % 3]
        
        # Filter valid models
        valid_models = [m for m in models_sorted 
                       if all_results['summary_explicit'][m][metric]['mean'] is not None 
                       or all_results['summary_implicit'][m][metric]['mean'] is not None]
        
        if valid_models:
            x_pos = np.arange(len(valid_models))
            width = 0.35
            
            explicit_means = [all_results['summary_explicit'][m][metric]['mean'] if all_results['summary_explicit'][m][metric]['mean'] is not None else 0 
                             for m in valid_models]
            implicit_means = [all_results['summary_implicit'][m][metric]['mean'] if all_results['summary_implicit'][m][metric]['mean'] is not None else 0 
                             for m in valid_models]
            
            # Plot side by side bars
            ax.bar(x_pos - width/2, explicit_means, width, alpha=0.7, label='Explicit', color='steelblue')
            ax.bar(x_pos + width/2, implicit_means, width, alpha=0.7, label='Implicit', color='coral')
            
            ax.set_xticks(x_pos)
            ax.set_xticklabels(valid_models, rotation=45, ha='right', fontsize=30)
            ax.set_ylabel(title, fontsize=30)
            ax.set_title(f'{title} (Explicit vs Implicit)', 
                        fontweight='bold' if metric == 'overall_score' else 'normal',
                        fontsize=36)
            ax.tick_params(axis='y', labelsize=26)
            ax.legend(fontsize=24)
            ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"model_comparison_explicit_vs_implicit_{timestamp}.png"), dpi=300, bbox_inches='tight')
    print(f"✓ Saved explicit vs implicit comparison plot")
    plt.close()
    
    # 3. Trajectory visualizations for each segment
    for segment_name, segment_results in all_results['by_segment'].items():
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # Plot GT trajectory
        first_result = list(segment_results.values())[0]
        gt_traj = np.array(first_result['gt_trajectory_2d'])
        ax.plot(gt_traj[:, 0], gt_traj[:, 1], 'k--', linewidth=3, label='Ground Truth', zorder=10)
        ax.scatter(gt_traj[0, 0], gt_traj[0, 1], c='green', s=200, marker='o', edgecolors='black', linewidth=2, label='GT Start', zorder=11)
        ax.scatter(gt_traj[-1, 0], gt_traj[-1, 1], c='red', s=200, marker='s', edgecolors='black', linewidth=2, label='GT End', zorder=11)
        
        # Collect all x values to adjust xlim and reduce left padding
        all_x_values = gt_traj[:, 0].tolist()
        
        # Plot predicted trajectories for each model (using consistent colors)
        for model_name, result in segment_results.items():
            pred_traj = np.array(result['trajectory_2d'])
            color = model_colors[model_name]  # Use consistent color from model_colors
            
            ax.plot(pred_traj[:, 0], pred_traj[:, 1], '-', color=color, linewidth=2, 
                   alpha=0.7, label=f'{model_name}')
            ax.scatter(pred_traj[0, 0], pred_traj[0, 1], c=color, s=80, marker='o', zorder=5)
            ax.scatter(pred_traj[-1, 0], pred_traj[-1, 1], c=color, s=80, marker='s', zorder=5)
            
            # Collect x values
            all_x_values.extend(pred_traj[:, 0].tolist())
        
        # Set xlim to reduce left empty space
        min_x = min(all_x_values)
        max_x = max(all_x_values)
        x_range = max_x - min_x
        ax.set_xlim(min_x - 0.01 * x_range, max_x + 0.05 * x_range)  # Minimal left margin
        
        ax.set_xlabel('X (meters)', fontsize=30)  # 2x bigger
        ax.set_ylabel('Y (meters)', fontsize=30)  # 2x bigger
        ax.set_title(f'Trajectory Comparison\n{segment_name}', fontsize=36, pad=20)  # Two rows, more padding
        ax.tick_params(axis='both', labelsize=26)  # 2x bigger for tick labels
        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=24)  # Legend outside on right
        ax.axis('equal')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"trajectory_{segment_name}_{timestamp}.png"), dpi=300, bbox_inches='tight')
        plt.close()
    
    print(f"✓ Saved {len(all_results['by_segment'])} trajectory comparison plots")


def main():
    """Main evaluation function."""
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description='Evaluate World Models Performance using World Decoder (VGGT Version) - Explicit vs Implicit'
    )
    parser.add_argument(
        '-n', '--num_samples',
        type=int,
        default=None,
        help='Number of samples to evaluate (default: all samples)'
    )
    parser.add_argument(
        '--segments_dir',
        type=str,
        default=None,
        help='Directory containing ground truth segments'
    )
    parser.add_argument(
        '--wm_videos_dir',
        type=str,
        default=None,
        help='Directory containing world model videos'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default=None,
        help='Directory to save evaluation results'
    )
    parser.add_argument(
        '--config_path',
        type=str,
        default=None,
        help='Path to VGGT configuration file'
    )
    parser.add_argument(
        '--eval_config_path',
        type=str,
        default=None,
        help='Path to evaluation metrics configuration file'
    )
    parser.add_argument(
        '--explicit_implicit_csv',
        type=str,
        default=None,
        help='Path to CSV file with explicit/implicit labels'
    )
    
    args = parser.parse_args()
    
    # Run evaluation
    results = evaluate_world_models(
        segments_dir=args.segments_dir,
        wm_videos_dir=args.wm_videos_dir,
        output_dir=args.output_dir,
        config_path=args.config_path,
        eval_config_path=args.eval_config_path,
        explicit_implicit_csv=args.explicit_implicit_csv,
        num_samples=args.num_samples
    )
    
    print("\n" + "="*80)
    print("Evaluation Complete!")
    print("="*80)
    output_dir_to_show = args.output_dir if args.output_dir else os.path.join(PROJECT_ROOT, "evaluation_results/vggt_implicit")
    print(f"Results saved to: {output_dir_to_show}")
    print("\nTo view results:")
    print(f"  - Summary CSV: {output_dir_to_show}/model_summary_*.csv")
    print(f"  - Detailed results: {output_dir_to_show}/segment_model_results_*.csv")
    print(f"  - Full JSON: {output_dir_to_show}/evaluation_results_*.json")
    print(f"  - Visualizations: {output_dir_to_show}/*.png")


if __name__ == "__main__":
    main()