#!/usr/bin/env python3
"""
Evaluate World Models Performance for 4s Horizon using World Decoder

This script:
1. Processes videos generated by world models for 4s horizon prediction
2. Extracts trajectories using the VGGT world decoder
3. Compares extracted trajectories with ground truth starting from 4s horizon point
4. Computes evaluation metrics (ADE, FDE, MR, SE, AC, Overall Score)
5. Generates comparison visualizations and reports

Directory structure expected:
- dataset/Benchmark/{segment_name}/ - Ground truth data
  - {segment_name}_current_frame.png
  - {segment_name}_4s.png (horizon 4s frame)
  - {segment_name}_metadata_4s.json
  - {segment_name}_trajectory_4s.csv (transformed trajectory CSV)
  
- dataset/wm_videos_horizon_at_4s/{model_name}/ - World model generated videos for 4s horizon
  - {segment_name}.mp4
"""

import os
import sys
import glob
import json
import yaml
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm.auto import tqdm
import torch
import cv2
import argparse

# Define project root and add paths
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# Add paths
sys.path.append(os.path.join(PROJECT_ROOT, 'pipelines/vggt'))
sys.path.append(os.path.join(PROJECT_ROOT, 'models/vggt'))

from extrinsic_path.extract_extrinsic_path import (
    extract_trajectory_from_extrinsics
)
from scale_factor.compute_scale_factor import (
    setup_model,
    load_config,
    extract_frames
)
from vggt.utils.load_fn import load_and_preprocess_images
from vggt.utils.pose_enc import pose_encoding_to_extri_intri

# Import trajectory metrics
sys.path.append(os.path.join(PROJECT_ROOT, 'evaluation'))
from metrics import compute_all_metrics


def load_evaluation_config(config_path=None):
    """
    Load evaluation configuration from YAML file.
    
    Args:
        config_path: Path to config YAML file
        
    Returns:
        dict: Configuration dictionary
    """
    if config_path is None:
        config_path = os.path.join(PROJECT_ROOT, "evaluation/config/vggt_config.yaml")
        
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def find_world_model_videos(wm_videos_dir=None):
    """
    Find all world model videos organized by model type.
    
    Returns:
        dict: {model_name: {segment_name: video_path}}
    """
    if wm_videos_dir is None:
        wm_videos_dir = os.path.join(PROJECT_ROOT, "dataset/wm_videos_horizon_at_4s")
        
    wm_videos = {}
    
    # Find all subdirectories in wm_videos (each is a model)
    if not os.path.exists(wm_videos_dir):
        print(f"Warning: World model videos directory not found: {wm_videos_dir}")
        return {}
        
    model_dirs = [d for d in glob.glob(os.path.join(wm_videos_dir, "*/")) if os.path.isdir(d)]
    
    for model_dir in model_dirs:
        model_name = os.path.basename(model_dir.rstrip('/'))
        wm_videos[model_name] = {}
        
        # Find all mp4 files in this model directory
        video_files = glob.glob(os.path.join(model_dir, "*.mp4"))
        
        for video_path in video_files:
            # Extract segment name from filename (remove .mp4 extension)
            segment_name = os.path.splitext(os.path.basename(video_path))[0]
            wm_videos[model_name][segment_name] = video_path
        
        print(f"Found {len(wm_videos[model_name])} videos for model: {model_name}")
    
    return wm_videos


def load_ground_truth_trajectory_4s(segment_dir, segment_name, metadata):
    """
    Load ground truth trajectory for 4s horizon evaluation.
    
    The ground truth starts from the horizon_4s point and includes all forward points.
    
    Args:
        segment_dir: Directory containing the segment data
        segment_name: Name of the segment
        metadata: Metadata dict loaded from metadata_4s.json
        
    Returns:
        numpy array: Ground truth trajectory (N, 2) or None if not found
    """
    trajectory_path = os.path.join(segment_dir, f"{segment_name}_trajectory_4s.csv")
    
    if not os.path.exists(trajectory_path):
        print(f"Warning: Trajectory file not found: {trajectory_path}")
        return None
    
    try:
        # Read the transformed trajectory CSV (trajectory_4s.csv)
        df = pd.read_csv(trajectory_path)
        
        # Get the starting index for 4s horizon
        # In pandas, this already accounts for header
        horizon_4s_index_relative = metadata["horizon_4s_frame_index_relative"]
        origin_csv_index = horizon_4s_index_relative
        
        # Get all points from horizon_4s point forward
        gt_trajectory = df.iloc[origin_csv_index:][['x', 'y']].values
        
        # Transform so that the first point (horizon_4s point) is at origin
        if len(gt_trajectory) > 0:
            origin_point = gt_trajectory[0].copy()
            gt_trajectory = gt_trajectory - origin_point
        
        print(f"  Ground truth trajectory: {len(gt_trajectory)} points from horizon_4s forward")
        
        return gt_trajectory
        
    except Exception as e:
        print(f"Error loading ground truth trajectory: {e}")
        import traceback
        traceback.print_exc()
        return None


def compute_trajectory_metrics(pred_trajectory_2d, gt_trajectory_2d, eval_config):
    """
    Compute trajectory comparison metrics as defined in metrics.md.
    
    Metrics:
    - ADE: Average Displacement Error
    - FDE: Final Displacement Error
    - MR: Miss Rate
    - SE: Soft Endpoint
    - AC: Approach Consistency
    - Overall Score: Weighted combination of all metrics
    
    Args:
        pred_trajectory_2d: Predicted trajectory (N, 2)
        gt_trajectory_2d: Ground truth trajectory (M, 2)
        eval_config: Evaluation configuration dict from YAML
        
    Returns:
        dict: Dictionary of metrics
    """
    # Handle empty trajectories
    if len(pred_trajectory_2d) == 0 or len(gt_trajectory_2d) == 0:
        return {
            'ade': float('inf'),
            'fde': float('inf'),
            'miss_rate': 100.0,
            'se': 0.0,
            'ac': 0.0,
            'overall_score': 0.0,
            'valid': False,
            'pred_points': len(pred_trajectory_2d),
            'gt_points': len(gt_trajectory_2d)
        }
    
    # Extract config parameters
    metrics_config = eval_config['metrics']
    overall_config = eval_config['overall_score']
    
    # Prepare AC parameters
    ac_params = metrics_config['ac']
    
    # Prepare overall score parameters
    overall_score_params = {
        'tau_ade': overall_config['tau_ade'],
        'tau_fde': overall_config['tau_fde'],
        'weights': overall_config['weights']
    }
    
    # Compute all metrics from metrics.py
    metrics = compute_all_metrics(
        pred_trajectory_2d, 
        gt_trajectory_2d,
        miss_threshold=metrics_config['miss_threshold'],
        interpolate_mode=metrics_config['interpolate_mode'],
        se_sigma=metrics_config['se_sigma'],
        ac_params=ac_params,
        overall_score_params=overall_score_params
    )
    
    metrics['valid'] = True
    
    return metrics


def evaluate_world_models(
    segments_dir=None,
    wm_videos_dir=None,
    output_dir=None,
    config_path=None,
    eval_config_path=None,
    num_samples=None
):
    """
    Evaluate all world models on all available segments for 4s horizon prediction.
    
    Args:
        segments_dir: Directory containing ground truth segments
        wm_videos_dir: Directory containing world model videos for 4s horizon
        output_dir: Directory to save evaluation results
        config_path: Path to VGGT configuration file
        eval_config_path: Path to evaluation metrics configuration file
        num_samples: Number of samples to evaluate (None for all samples)
        
    Returns:
        dict: Complete evaluation results
    """
    # Set default paths
    if segments_dir is None:
        segments_dir = os.path.join(PROJECT_ROOT, "dataset/Benchmark")
    if wm_videos_dir is None:
        wm_videos_dir = os.path.join(PROJECT_ROOT, "dataset/wm_videos_horizon_at_4s")
    if output_dir is None:
        output_dir = os.path.join(PROJECT_ROOT, "evaluation_results/vggt_horizon_4s")
    if config_path is None:
        config_path = os.path.join(PROJECT_ROOT, "pipelines/vggt/configs/vggt_pcd_configs.yml")
    if eval_config_path is None:
        eval_config_path = os.path.join(PROJECT_ROOT, "evaluation/config/vggt_config.yaml")

    print("="*80)
    print("World Models Evaluation (4s Horizon) using World Decoder")
    print("="*80)
    
    # Load evaluation configuration
    eval_config = load_evaluation_config(eval_config_path)
    print(f"\nLoaded evaluation config from: {eval_config_path}")
    print(f"  Miss threshold: {eval_config['metrics']['miss_threshold']}m")
    print(f"  Overall score weights: ADE={eval_config['overall_score']['weights']['ade']}, "
          f"FDE={eval_config['overall_score']['weights']['fde']}, "
          f"MR={eval_config['overall_score']['weights']['mr']}, "
          f"SE={eval_config['overall_score']['weights']['se']}, "
          f"AC={eval_config['overall_score']['weights']['ac']}")
    
    # Load VGGT configuration
    vggt_config = load_config(config_path)
    
    # Setup VGGT model ONCE before processing all videos
    print("\n" + "="*80)
    print("Loading VGGT Model (one-time setup)")
    print("="*80)
    model, device, dtype = setup_model()
    print(f"✓ Model loaded successfully on {device} with dtype {dtype}")
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Find all segments
    segment_dirs = sorted([d for d in glob.glob(os.path.join(segments_dir, "*/")) if os.path.isdir(d)])
    print(f"\nFound {len(segment_dirs)} segments in total")
    
    # Limit number of samples if specified
    if num_samples is not None and num_samples > 0:
        segment_dirs = segment_dirs[:num_samples]
        print(f"Limiting evaluation to {len(segment_dirs)} samples (--num_samples={num_samples})")
    else:
        print(f"Evaluating all {len(segment_dirs)} segments")
    
    # Find all world model videos
    wm_videos = find_world_model_videos(wm_videos_dir)
    print(f"\nFound {len(wm_videos)} world models: {list(wm_videos.keys())}")
    
    # Initialize results storage
    all_results = {
        'by_model': {},
        'by_segment': {},
        'summary': {}
    }
    
    # Process each model
    for model_name, model_videos in wm_videos.items():
        print(f"\n{'='*80}")
        print(f"Evaluating Model: {model_name}")
        print(f"{'='*80}")
        
        all_results['by_model'][model_name] = {}
        
        # Process each segment for this model
        for segment_dir in tqdm(segment_dirs, desc=f"Processing {model_name}"):
            segment_name = os.path.basename(segment_dir.rstrip('/'))
            
            # Check if we have a video for this segment
            if segment_name not in model_videos:
                print(f"Warning: No video found for segment {segment_name} in model {model_name}")
                continue
            
            wm_video_path = model_videos[segment_name]
            
            print(f"\nProcessing: {segment_name}")
            print(f"  WM Video: {os.path.basename(wm_video_path)}")
            
            # === INLINE PROCESSING LOGIC (using pre-loaded model) ===
            try:
                # Construct file paths for 4s horizon
                current_frame_path = os.path.join(segment_dir, f"{segment_name}_current_frame.png")
                horizon_4s_frame_path = os.path.join(segment_dir, f"{segment_name}_4s.png")
                metadata_path = os.path.join(segment_dir, f"{segment_name}_metadata_4s.json")
                
                # Check if required files exist
                if not all(os.path.exists(path) for path in [current_frame_path, horizon_4s_frame_path, metadata_path]):
                    print(f"  ✗ Missing required files")
                    continue
                
                # Check if video exists
                if not os.path.exists(wm_video_path):
                    print(f"  ✗ Video path does not exist: {wm_video_path}")
                    continue
                
                # Load metadata to get real displacement
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                
                real_displacement = metadata["distance_current_to_4s_meters"]
                
                # Setup camera paths: current frame + predicted frames from world model video
                camera_paths = []
                
                # Extract frames from world model generated video
                temp_frame_dir = os.path.join(output_dir, 'video_frames')
                os.makedirs(temp_frame_dir, exist_ok=True)
                predicted_image_paths = extract_frames(wm_video_path, temp_frame_dir)
                
                camera_paths.extend(predicted_image_paths)
                camera_paths.append(current_frame_path)
                
                # Load and preprocess images
                images = load_and_preprocess_images(camera_paths).to(device)
                
                # Run VGGT inference with pre-loaded model
                with torch.no_grad():
                    with torch.cuda.amp.autocast(dtype=dtype):
                        predictions = model(images)
                    
                        images = images[None]  # add batch dimension
                        aggregated_tokens_list, ps_idx = model.aggregator(images)
                                
                    # Predict Cameras
                    pose_enc = model.camera_head(aggregated_tokens_list)[-1]
                    # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)
                    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])
                    
                    # Extract camera positions from extrinsic matrices
                    # First camera: reference frame (current_frame.png)
                    # Last camera: last predicted frame from world model
                    camera1_position = extrinsic[0, 0, :3, 3]  # Current frame position
                    camera2_position = extrinsic[0, -1, :3, 3]  # Last predicted frame position
                    
                    # Calculate Euclidean distance between camera positions
                    camera_displacement = torch.norm(camera1_position - camera2_position).item()
                
                # Compute scale factor
                if camera_displacement == 0:
                    print(f"  ✗ Zero camera displacement")
                    continue
                else:
                    scale_factor = real_displacement / camera_displacement
                
                # Extract trajectory from extrinsic matrices
                trajectory, orientations, trajectory_2d = extract_trajectory_from_extrinsics(
                    extrinsic.cpu().numpy(), 
                    scale_factor
                )
                
                trajectory_length_meters = np.sum(np.linalg.norm(np.diff(trajectory, axis=0), axis=1))
                
            except Exception as e:
                print(f"  ✗ Error processing: {e}")
                import traceback
                traceback.print_exc()
                continue
            # === END INLINE PROCESSING ===
            
            # Load ground truth trajectory for 4s horizon
            gt_trajectory_2d = load_ground_truth_trajectory_4s(segment_dir, segment_name, metadata)
            
            if gt_trajectory_2d is None:
                print(f"  ✗ No ground truth trajectory found")
                continue
            
            # Compute metrics
            pred_trajectory_2d = trajectory_2d
            metrics = compute_trajectory_metrics(pred_trajectory_2d, gt_trajectory_2d, eval_config)
            
            # Store results
            result_entry = {
                'segment_name': segment_name,
                'model_name': model_name,
                'scale_factor': float(scale_factor),
                'real_displacement': float(real_displacement),
                'camera_displacement': float(camera_displacement),
                'trajectory_length': float(trajectory_length_meters),
                'metrics': metrics,
                'trajectory': trajectory.tolist(),
                'trajectory_2d': pred_trajectory_2d.tolist(),
                'gt_trajectory_2d': gt_trajectory_2d.tolist()
            }
            
            all_results['by_model'][model_name][segment_name] = result_entry
            
            # Also store by segment for cross-model comparison
            if segment_name not in all_results['by_segment']:
                all_results['by_segment'][segment_name] = {}
            all_results['by_segment'][segment_name][model_name] = result_entry
            
            print(f"  ✓ Overall: {metrics['overall_score']:.4f} | ADE: {metrics['ade']:.4f}m, FDE: {metrics['fde']:.4f}m, MR: {metrics['miss_rate']:.2f}%, SE: {metrics['se']:.4f}, AC: {metrics['ac']:.4f}")
    
    # Compute summary statistics
    print(f"\n{'='*80}")
    print("Computing Summary Statistics")
    print(f"{'='*80}")
    
    for model_name, model_results in all_results['by_model'].items():
        if not model_results:
            continue
        
        # Collect all metrics for this model (only metrics from metrics.md)
        ade_values = []
        fde_values = []
        miss_rates = []
        se_values = []
        ac_values = []
        overall_scores = []
        
        for segment_result in model_results.values():
            metrics = segment_result['metrics']
            if metrics['valid']:
                ade_values.append(metrics['ade'])
                fde_values.append(metrics['fde'])
                miss_rates.append(metrics['miss_rate'])
                se_values.append(metrics['se'])
                ac_values.append(metrics['ac'])
                overall_scores.append(metrics['overall_score'])
        
        # Compute summary statistics
        summary = {
            'num_segments': len(model_results),
            'num_valid': len(ade_values),
            'overall_score': {
                'mean': float(np.mean(overall_scores)) if overall_scores else None,
                'median': float(np.median(overall_scores)) if overall_scores else None,
                'std': float(np.std(overall_scores)) if overall_scores else None,
                'min': float(np.min(overall_scores)) if overall_scores else None,
                'max': float(np.max(overall_scores)) if overall_scores else None
            },
            'ade': {
                'mean': float(np.mean(ade_values)) if ade_values else None,
                'median': float(np.median(ade_values)) if ade_values else None,
                'std': float(np.std(ade_values)) if ade_values else None,
                'min': float(np.min(ade_values)) if ade_values else None,
                'max': float(np.max(ade_values)) if ade_values else None
            },
            'fde': {
                'mean': float(np.mean(fde_values)) if fde_values else None,
                'median': float(np.median(fde_values)) if fde_values else None,
                'std': float(np.std(fde_values)) if fde_values else None,
                'min': float(np.min(fde_values)) if fde_values else None,
                'max': float(np.max(fde_values)) if fde_values else None
            },
            'miss_rate': {
                'mean': float(np.mean(miss_rates)) if miss_rates else None,
                'median': float(np.median(miss_rates)) if miss_rates else None,
                'std': float(np.std(miss_rates)) if miss_rates else None,
                'min': float(np.min(miss_rates)) if miss_rates else None,
                'max': float(np.max(miss_rates)) if miss_rates else None
            },
            'se': {
                'mean': float(np.mean(se_values)) if se_values else None,
                'median': float(np.median(se_values)) if se_values else None,
                'std': float(np.std(se_values)) if se_values else None,
                'min': float(np.min(se_values)) if se_values else None,
                'max': float(np.max(se_values)) if se_values else None
            },
            'ac': {
                'mean': float(np.mean(ac_values)) if ac_values else None,
                'median': float(np.median(ac_values)) if ac_values else None,
                'std': float(np.std(ac_values)) if ac_values else None,
                'min': float(np.min(ac_values)) if ac_values else None,
                'max': float(np.max(ac_values)) if ac_values else None
            }
        }
        
        all_results['summary'][model_name] = summary
        
        print(f"\n{model_name}:")
        print(f"  Segments processed: {summary['num_valid']}/{summary['num_segments']}")
        if summary['overall_score']['mean']:
            print(f"  Overall Score: {summary['overall_score']['mean']:.4f} ± {summary['overall_score']['std']:.4f}")
            print(f"  ADE: {summary['ade']['mean']:.4f} ± {summary['ade']['std']:.4f} m")
            print(f"  FDE: {summary['fde']['mean']:.4f} ± {summary['fde']['std']:.4f} m")
            print(f"  Miss Rate (2m): {summary['miss_rate']['mean']:.2f} ± {summary['miss_rate']['std']:.2f} %")
            print(f"  SE (Soft Endpoint): {summary['se']['mean']:.4f} ± {summary['se']['std']:.4f}")
            print(f"  AC (Approach Consistency): {summary['ac']['mean']:.4f} ± {summary['ac']['std']:.4f}")
    
    # Save results to JSON
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    results_json_path = os.path.join(output_dir, f"evaluation_results_horizon_4s_{timestamp}.json")
    
    # Convert numpy types for JSON serialization
    def convert_for_json(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.int32, np.int64)):
            return int(obj)
        return obj
    
    with open(results_json_path, 'w') as f:
        json.dump(all_results, f, indent=2, default=convert_for_json)
    
    print(f"\n✓ Saved detailed results to: {results_json_path}")
    
    # Create summary CSV
    create_summary_csv(all_results, output_dir, timestamp)
    
    # Create visualizations
    try:
        create_evaluation_visualizations(all_results, output_dir, timestamp)
    except Exception as e:
        print(f"Warning: Could not create visualizations: {e}")
    
    return all_results


def create_summary_csv(all_results, output_dir, timestamp):
    """Create summary CSV files for easy analysis."""
    
    # 1. Per-segment, per-model results
    segment_model_data = []
    for segment_name, segment_results in all_results['by_segment'].items():
        for model_name, result in segment_results.items():
            metrics = result['metrics']
            row = {
                'segment': segment_name,
                'model': model_name,
                'scale_factor': result['scale_factor'],
                'real_displacement': result['real_displacement'],
                'trajectory_length': result['trajectory_length'],
                'overall_score': metrics['overall_score'],
                'ade': metrics['ade'],
                'fde': metrics['fde'],
                'miss_rate': metrics['miss_rate'],
                'se': metrics['se'],
                'ac': metrics['ac'],
                'pred_points': metrics['pred_points'],
                'gt_points': metrics['gt_points']
            }
            segment_model_data.append(row)
    
    df_segment_model = pd.DataFrame(segment_model_data)
    csv_path = os.path.join(output_dir, f"segment_model_results_horizon_4s_{timestamp}.csv")
    df_segment_model.to_csv(csv_path, index=False)
    print(f"✓ Saved per-segment results to: {csv_path}")
    
    # 2. Model summary statistics
    summary_data = []
    for model_name, summary in all_results['summary'].items():
        row = {
            'model': model_name,
            'num_segments': summary['num_segments'],
            'num_valid': summary['num_valid'],
            'overall_score_mean': summary['overall_score']['mean'],
            'overall_score_std': summary['overall_score']['std'],
            'ade_mean': summary['ade']['mean'],
            'ade_std': summary['ade']['std'],
            'fde_mean': summary['fde']['mean'],
            'fde_std': summary['fde']['std'],
            'miss_rate_mean': summary['miss_rate']['mean'],
            'miss_rate_std': summary['miss_rate']['std'],
            'se_mean': summary['se']['mean'],
            'se_std': summary['se']['std'],
            'ac_mean': summary['ac']['mean'],
            'ac_std': summary['ac']['std']
        }
        summary_data.append(row)
    
    df_summary = pd.DataFrame(summary_data)
    csv_path = os.path.join(output_dir, f"model_summary_horizon_4s_{timestamp}.csv")
    df_summary.to_csv(csv_path, index=False)
    print(f"✓ Saved model summary to: {csv_path}")


def create_evaluation_visualizations(all_results, output_dir, timestamp):
    """Create comprehensive visualizations of evaluation results."""
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    
    # Sort models: gt_video first, then others alphabetically
    models = list(all_results['summary'].keys())
    models_sorted = sorted(models, key=lambda x: (x != 'gt_video', x))
    
    # Create consistent color mapping for all models
    # Use tab20 for more distinct colors
    color_palette = cm.tab20(np.linspace(0, 1, 20))
    model_colors = {}
    for i, model_name in enumerate(models_sorted):
        model_colors[model_name] = color_palette[i % 20]
    
    # 1. Model comparison - bar charts (using metrics from metrics.md + overall score)
    fig, axes = plt.subplots(2, 3, figsize=(22, 16))
    
    metrics_to_plot = ['overall_score', 'ade', 'fde', 'miss_rate', 'se', 'ac']
    titles = ['Overall Score', 'ADE (m)', 'FDE (m)', 'Miss Rate (%)', 'SE', 'AC']
    
    for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):
        ax = axes[idx // 3, idx % 3]
        
        # Filter valid models and sort (gt_video first)
        valid_models = [m for m in models_sorted if all_results['summary'][m][metric]['mean'] is not None]
        means = [all_results['summary'][m][metric]['mean'] for m in valid_models]
        
        if means:
            x_pos = np.arange(len(valid_models))
            # Get colors for each model
            colors = [model_colors[m] for m in valid_models]
            
            # Plot without error bars
            ax.bar(x_pos, means, alpha=0.7, color=colors)
            ax.set_xticks(x_pos + 0.4)  # Shift ticks slightly right
            ax.set_xticklabels(valid_models, rotation=75, ha='right', fontsize=30)
            ax.set_title(f'{title}', 
                        fontweight='bold' if metric == 'overall_score' else 'normal',
                        fontsize=36)
            ax.tick_params(axis='y', labelsize=26)
            ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.subplots_adjust(wspace=0.15)
    plt.savefig(os.path.join(output_dir, f"model_comparison_horizon_4s_{timestamp}.png"), dpi=300, bbox_inches='tight')
    print(f"✓ Saved model comparison plot")
    plt.close()
    
    # 2. Trajectory visualizations for each segment
    for segment_name, segment_results in all_results['by_segment'].items():
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # Plot GT trajectory
        first_result = list(segment_results.values())[0]
        gt_traj = np.array(first_result['gt_trajectory_2d'])
        ax.plot(gt_traj[:, 0], gt_traj[:, 1], 'k--', linewidth=3, label='Ground Truth', zorder=10)
        ax.scatter(gt_traj[0, 0], gt_traj[0, 1], c='green', s=200, marker='o', edgecolors='black', linewidth=2, label='GT Start', zorder=11)
        ax.scatter(gt_traj[-1, 0], gt_traj[-1, 1], c='red', s=200, marker='s', edgecolors='black', linewidth=2, label='GT End', zorder=11)
        
        # Collect all x values to adjust xlim and reduce left padding
        all_x_values = gt_traj[:, 0].tolist()
        
        # Plot predicted trajectories for each model (using consistent colors)
        for model_name, result in segment_results.items():
            pred_traj = np.array(result['trajectory_2d'])
            color = model_colors[model_name]
            
            ax.plot(pred_traj[:, 0], pred_traj[:, 1], '-', color=color, linewidth=2, 
                   alpha=0.7, label=f'{model_name}')
            ax.scatter(pred_traj[0, 0], pred_traj[0, 1], c=color, s=80, marker='o', zorder=5)
            ax.scatter(pred_traj[-1, 0], pred_traj[-1, 1], c=color, s=80, marker='s', zorder=5)
            
            # Collect x values
            all_x_values.extend(pred_traj[:, 0].tolist())
        
        # Set xlim to reduce left empty space
        min_x = min(all_x_values)
        max_x = max(all_x_values)
        x_range = max_x - min_x
        ax.set_xlim(min_x - 0.01 * x_range, max_x + 0.05 * x_range)  # Minimal left margin
        
        ax.set_xlabel('X (meters)', fontsize=30)
        ax.set_ylabel('Y (meters)', fontsize=30)
        ax.set_title(f'Trajectory Comparison\n4s Horizon - {segment_name}', fontsize=36, pad=20)  # Two rows, more padding
        ax.tick_params(axis='both', labelsize=26)
        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=24)  # Legend outside on right
        ax.axis('equal')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"trajectory_horizon_4s_{segment_name}_{timestamp}.png"), dpi=300, bbox_inches='tight')
        plt.close()
    
    print(f"✓ Saved {len(all_results['by_segment'])} trajectory comparison plots")


def main():
    """Main evaluation function."""
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description='Evaluate World Models Performance for 4s Horizon using World Decoder (VGGT Version)'
    )
    parser.add_argument(
        '-n', '--num_samples',
        type=int,
        default=None,
        help='Number of samples to evaluate (default: all samples)'
    )
    parser.add_argument(
        '--segments_dir',
        type=str,
        default=None,
        help='Directory containing ground truth segments'
    )
    parser.add_argument(
        '--wm_videos_dir',
        type=str,
        default=None,
        help='Directory containing world model videos for 4s horizon'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default=None,
        help='Directory to save evaluation results'
    )
    parser.add_argument(
        '--config_path',
        type=str,
        default=None,
        help='Path to VGGT configuration file'
    )
    parser.add_argument(
        '--eval_config_path',
        type=str,
        default=None,
        help='Path to evaluation metrics configuration file'
    )
    
    args = parser.parse_args()
    
    # Run evaluation
    results = evaluate_world_models(
        segments_dir=args.segments_dir,
        wm_videos_dir=args.wm_videos_dir,
        output_dir=args.output_dir,
        config_path=args.config_path,
        eval_config_path=args.eval_config_path,
        num_samples=args.num_samples
    )
    
    print("\n" + "="*80)
    print("Evaluation Complete!")
    print("="*80)
    output_dir_to_show = args.output_dir if args.output_dir else os.path.join(PROJECT_ROOT, "evaluation_results/vggt_horizon_4s")
    print(f"Results saved to: {output_dir_to_show}")
    print("\nTo view results:")
    print(f"  - Summary CSV: {output_dir_to_show}/model_summary_horizon_4s_*.csv")
    print(f"  - Detailed results: {output_dir_to_show}/segment_model_results_horizon_4s_*.csv")
    print(f"  - Full JSON: {output_dir_to_show}/evaluation_results_horizon_4s_*.json")
    print(f"  - Visualizations: {output_dir_to_show}/*.png")


if __name__ == "__main__":
    main()